{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import gensim\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import Imputer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# import os\n",
    "# import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question2words(question, stops):\n",
    "    \"\"\"\n",
    "    :param question: single question string\n",
    "    :return:\n",
    "    This function converts a raw question to a string of words\n",
    "    \"\"\"\n",
    "    # remove non-letters => C vs C++\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", question)\n",
    "\n",
    "    # tagging\n",
    "    tags = TextBlob(letters_only).tags\n",
    "\n",
    "    # convert to lower case, split into separate words\n",
    "    words = letters_only.lower().split(\" \")\n",
    "\n",
    "    # remove stop words\n",
    "    #meaningful_words = [w for w in words if (not (w in stops or len(w)<2))]\n",
    "    tags = [t for t,w in zip(tags,words) if (not (w in stops or len(w)<2))]\n",
    "\n",
    "    # return an array of meaningful words\n",
    "    return (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qwords2vector(words, model, index2word_set ):\n",
    "    \"\"\"\n",
    "    Function to average all of the word vectors in a given question\n",
    "\n",
    "    :param words:\n",
    "    :param model:\n",
    "    :param index2word_set:\n",
    "    :param num_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((NUM_FEATURES),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "\n",
    "    # Loop over each word in the question and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0:\n",
    "        return featureVec\n",
    "\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qwords2vectorOfvectors(words, model, index2word_set ):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to average all of the word vectors in a given question\n",
    "\n",
    "    :param words:\n",
    "    :param model:\n",
    "    :param index2word_set:\n",
    "    :param num_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return [(model[w[0]],w[1]) for w in words if w[0] in index2word_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x,y):\n",
    "    \n",
    "    result = 1 - spatial.distance.cosine(x, y)\n",
    "    \n",
    "    #x_abs = math.sqrt(sum([i*i for i in x]))\n",
    "    #y_abs = math.sqrt(sum([i*i for i in y]))\n",
    "    #normal_factor = x_abs * y_abs\n",
    "\n",
    "    #if normal_factor == 0:\n",
    "        #return 0.0\n",
    "\n",
    "    # result = sum([abs(x1-y1) for x1,y1 in zip(x,y)])/normal_factor\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_dis(x,y):\n",
    "    return [abs(x1-y1) for x1,y1 in zip(x,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def argmax(lst):\n",
    "    if len(lst) == 0:\n",
    "        return -1\n",
    "    return lst.index(max(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def find_best_matched(word, question):\n",
    "def find_best_matched(vector_word, vectors):\n",
    "    \n",
    "    return argmax([ cosine_similarity( vector_word[0] , vec[0] ) for vec in vectors])\n",
    "\n",
    "    #return argmax([cos_dis(word[0],pair[0]) for pair in question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def weighted_pair_average_dist_vecOfvec(short_q_v,long_q_v,short_q_words,long_q_words,num_features): #vector of vectors, and vector of words\n",
    "#def weighted_pair_average_dist_vecOfvec(vectors_short, vecors_long, words_short, words_long ): #vector of vectors, and vector of words\n",
    "def weighted_average_word_vectors_distance(vectors_short, vectors_long, words_short, words_long ): #vector of vectors, and vector of words\n",
    "\n",
    "    # for the shortest sentence\n",
    "        # for each word => find the pair in the other sentence\n",
    "\n",
    "    i = 0\n",
    "#    pair_word_idxes =[]\n",
    "#    pair_words =[]\n",
    "#    pair_word_vecs =[]\n",
    "    index_pair =[]\n",
    "    words_pair =[]\n",
    "    vectors_pair =[]\n",
    "    \n",
    "#    for word_v in short_q_v:\n",
    "#        pair_word_idx = find_best_matched(word_v, long_q_v)\n",
    "#        if pair_word_idx != -1:\n",
    "#            pair_word_idxes.append(pair_word_idx)\n",
    "#            pair_words.append(long_q_words[pair_word_idx])\n",
    "#            pair_word_vecs.append(long_q_v[pair_word_idx])     \n",
    "    for vector in vectors_short:\n",
    "        \n",
    "        match_index = find_best_matched(vector, vectors_long)\n",
    "        \n",
    "        if match_index != -1:\n",
    "            \n",
    "            index_pair.append(match_index)\n",
    "            \n",
    "            words_pair.append(words_long[match_index])\n",
    "            \n",
    "            vectors_pair.append(vecors_long[match_index])\n",
    "\n",
    "    # find [abs(x1-y1) for x1,y1 in zip(x,y)] for each words and its pair => sum\n",
    "    #q1_q2_words_dist = [abs_dis(x1,y1) for x1,y1 in zip(short_q_v,pair_word_vecs)]\n",
    "    #q1_q2_pos_matching = [0.75 if x[1] == y[1] else 0.25 for x,y in zip(short_q_words,pair_words)]\n",
    "    \n",
    "    #q1_q2_words_dist = [abs_dis(x,y) for x,y in zip(vectors_short , vectors_pair)]\n",
    "    \n",
    "    #q1_q2_pos_matching = [0.75 if x[1] == y[1] else 0.25 for x,y in zip(words_short , words_pair)]\n",
    "    \n",
    "    dist = [abs_dis(x,y) for x,y in zip(vectors_short , vectors_pair)]\n",
    "    \n",
    "    pos_matching = [0.75 if x[1] == y[1] else 0.25 for x,y in zip(words_short , words_pair)]\n",
    "\n",
    "    #return avg_vecOfvec(q1_q2_words_dist,q1_q2_pos_matching, num_features)\n",
    "    return weighted_average( dist, pos_matching )\n",
    "    #return avg_vecOfvec( dist, pos_matching )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def avg_vecOfvec(q1_q2_words_dist, q1_q2_pos_matching ):\n",
    "def weighted_average( vector_word_distance , vector_pos_matching ):\n",
    "\n",
    "    # average between all the available vectors\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((NUM_FEATURES),dtype=\"float32\")\n",
    "\n",
    "    #\n",
    "    nwords = 0\n",
    "\n",
    "    for dist, weight in zip(vector_word_distance , vector_pos_matching):\n",
    "        \n",
    "        featureVec = np.add(featureVec, dist * weight)\n",
    "        \n",
    "        nwords += weight\n",
    "\n",
    "    #for word_dst, weight in zip(q1_q2_words_dist,q1_q2_pos_matching):\n",
    "    #    featureVec = np.add(featureVec,word_dst*weight)\n",
    "    #    nwords += weight\n",
    "    #\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords == 0:\n",
    "        return featureVec\n",
    "\n",
    "    result = featureVec = np.divide(featureVec,nwords)\n",
    "    \n",
    "    #result = [ np.average(x, weights=y) for x,y in zip(vector_word_distance, vector_pos_matching )]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6de1293a4e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;31m## embedding model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    231\u001b[0m                             \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                     \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main\n",
    "\n",
    "    ## embedding model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    ## read data\n",
    "#train_data = pd.read_csv(\"./data/train1000.csv\")\n",
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\mmoham12/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Programs\\\\Anaconda3\\\\nltk_data'\n    - 'D:\\\\Programs\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mmoham12\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\mmoham12/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Programs\\\\Anaconda3\\\\nltk_data'\n    - 'D:\\\\Programs\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mmoham12\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c70d87fc28b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;31m# In Python, searching a set is much faster than searching\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;31m#   a list, so convert the stop words to a set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mstops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\mmoham12/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Programs\\\\Anaconda3\\\\nltk_data'\n    - 'D:\\\\Programs\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mmoham12\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "    ## separate input and result\n",
    "\n",
    "    # columns: ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
    "train_id = train_data['id']\n",
    "train_qid1 = train_data['qid1']\n",
    "train_qid2 = train_data['qid2']\n",
    "train_question1 = train_data['question1']\n",
    "train_question2 = train_data['question2']\n",
    "\n",
    "train_is_duplicate = train_data['is_duplicate']\n",
    "\n",
    "\n",
    "    # columns: ['id', 'qid1', 'qid2', 'question1', 'question2']\n",
    "    # test_id = test_data['test_id']\n",
    "    # test_question1 = test_data['question1']\n",
    "    # test_question2 = test_data['question2']\n",
    "\n",
    "    ## clean the input\n",
    "    # In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_question1_word_tags = [question2words(str(x), stops) for x in train_question1]\n",
    "train_question2_word_tags = [question2words(str(x), stops) for x in train_question2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_question1_word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-e63ef8b5edcc>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-e63ef8b5edcc>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    short_q_v, long_q_v, short_q_words, long_q_words) for\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "# clean_test_question1 = [question2words(str(x),stops) for x in test_question1]\n",
    "    # clean_test_question2 = [question2words(str(x),stops) for x in test_question2]\n",
    "\n",
    "    # removing repeated words?\n",
    "\n",
    "    # separate short from large\n",
    "    clean_train_short_q = [x if len(x) < len(y) else y for x,y in zip(train_question1_word_tags, train_question2_word_tags)]\n",
    "    clean_train_long_q = [x if len(x) > len(y) else y for x,y in zip(train_question1_word_tags, train_question2_word_tags)]\n",
    "\n",
    "\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    #num_features = 300\n",
    "\n",
    "    # convert to vectors of vectors\n",
    "    vectorsOfvector_train_short_q = [words2vectorOfvectors(x, model, index2word_set) for x in clean_train_short_q]\n",
    "    vectorsOfvector_train_long_q = [words2vectorOfvectors(x, model, index2word_set) for x in clean_train_long_q]\n",
    "\n",
    "\n",
    "    ## compute the distance of question 1 and question 2\n",
    "    \n",
    "    #train_distance_q1_q2 = [ weighted_pair_average_dist_vecOfvec(\n",
    "    #short_q_v, long_q_v, short_q_words, long_q_words) for \n",
    "                           # short_q_v, long_q_v,  short_q_words, long_q_words in zip(\n",
    "                           #     vectorsOfvector_train_short_q, vectorsOfvector_train_long_q, \n",
    "                           #     clean_train_short_q, clean_train_long_q)]\n",
    "        \n",
    "    train_distance_q1_q2 = [ \n",
    "        weighted_average_word_vectors_distance( vector_short, vector_long, words_short, words_long) for \n",
    "                            vector_short , vector_long,  words_short, words_long\n",
    "                            in zip(\n",
    "                                vectorsOfvector_train_short_q, vectorsOfvector_train_long_q, \n",
    "                                clean_train_short_q, clean_train_long_q)\n",
    "                           ]\n",
    "\n",
    "    # test_distance_q1_q2 = [dot(x,y) for x,y in zip(vectors_test_question1,vectors_test_question2)]\n",
    "\n",
    "    # just for now, because of NAN error, should be resolved in better way\n",
    "    train_features = train_distance_q1_q2 #\n",
    "    #Imputer().fit_transform(train_distance_q1_q2)\n",
    "\n",
    "    train_result = train_is_duplicate\n",
    "\n",
    "    # test_features = test_distance_q1_q2 # Imputer().fit_transform(test_distance_q1_q2)\n",
    "\n",
    "    # as we only have the label of train data, we test it by cross validation in train data\n",
    "    #train_train_data, train_test_data, train_train_result, train_test_result = \n",
    "        #cross_validation.train_test_split(train_features,train_result, test_size = 0.2, random_state = 42322)\n",
    "        \n",
    "    X = train_features \n",
    "    y = train_result\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.2, random_state = 42322)\n",
    "\n",
    "    # Fit a random forest to the training data, using 1000 trees\n",
    "    #forest = RandomForestClassifier(n_estimators = 100)\n",
    "    classifier = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "    print(\"Fitting a random forest to labeled training data...\")\n",
    "    #classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "    #score_tr_tr = forest.score(X_train, y_train) #\n",
    "    #score_tr_tst = forest.score(X_test, y_test) #\n",
    "    \n",
    "    score_acrcy_X_train = classifier.score(X_train, y_train) \n",
    "    score_acrcy_X_test = classifier.score(X_test, y_test) # Because the lables are binary,the score is Jaccard Dist:J(X,Y)=|X∩Y|/|X∪Y|\n",
    "\n",
    "    #result_prob_tr_tr = forest.predict_proba(train_train_data)\n",
    "    #result_prob_tr_tst = forest.predict_proba(train_test_data)\n",
    "    predict_X_train = classifier.predict_proba(X_train) # It has NOT used\n",
    "    predict_X_test = classifier.predict_proba(X_test)\n",
    "\n",
    "    #false_positive_rate, true_positive_rate, thresholds = roc_curve(train_test_result, result_prob_tr_tst[:, 1])\n",
    "    #roc_auc = auc(false_positive_rate, true_positive_rate) #0.66166516195753156\n",
    "    \n",
    "    fp, tp, thresholds = roc_curve(y_test, predict_X_test[:, 1])\n",
    "    score_auc_X_test = auc(fp, tp) \n",
    "    #0.66166516195753156 , calculating area under the curt when the 'fp' as the X-axis and 'tp' as y-axis\n",
    "    \n",
    "\n",
    "    #print(\"train score = %s, test score = %s, roc_auc = %s\",str(score_tr_tr),str(score_tr_tst),str(roc_auc))\n",
    "    print(\"Train Jaccard Score = %s, Test Jaccard Score = %s, TEST AUC Score = %s\",\n",
    "              str(score_acrcy_X_train),str(score_acrcy_X_test),str(score_auc_X_test))\n",
    "   \n",
    "\n",
    "    # Prediction Fist and then Scoring using KFlod corss-validation (for 10 partitions)\n",
    "    predicted = cross_val_predict(classifier, X, y, cv=10)\n",
    "    \n",
    "    score_accuracy= metrics.accuracy_score(y, predicted) \n",
    "    score_auc= metrics.auc(y, predicted) \n",
    "\n",
    "       \n",
    "    print(\"Accuracy: %0.7f \" % (score_accuracy))\n",
    "    print(\"AUC : %0.7f \" % (score_auc))\n",
    "    \n",
    "        \n",
    "    # Scoring using KFlod corss-validation (for 10 partitions)\n",
    "    score_f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1').mean() \n",
    "    scores_auc = cross_val_score(classifier, X, y, cv=10, scoring='roc_auc').mean()\n",
    "    \n",
    "      \n",
    "    print(\"Accuracy: %0.7f \" % (score_f1))\n",
    "    print(\"AUC : %0.7f \" % (scores_auc))\n",
    " \n",
    "    \n",
    "    ## here is the main testing for the main result ##\n",
    "    # Fit a random forest to the training data, using 1000 trees\n",
    "    # forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "    # print(\"Fitting a random forest to labeled training data...\")\n",
    "    # forest = forest.fit(train_features, train_result)\n",
    "\n",
    "    # result_tst = forest.predict(test_features)\n",
    "    # result_prob_tst = forest.predict_proba(test_features)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    status = main()\n",
    "    sys.exit(status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
